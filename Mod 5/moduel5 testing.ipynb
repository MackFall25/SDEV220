{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ac5c29-e5c1-4a49-9081-c2869b35296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sum(iterable):\n",
    "    total = 0\n",
    "    for v in iterable:\n",
    "        total += v\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79993b8d-85c4-4495-a886-69906c9a2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMySum(unittest.TestCase):\n",
    "    def test_list_int(self):\n",
    "        data = [1, 2, 3]\n",
    "        result = my_sum(data)\n",
    "        self.assertEqual(result, 6)\n",
    "\n",
    "    def test_tuple(self):\n",
    "        data = (4, 5, 6)\n",
    "        result = my_sum(data)\n",
    "        self.assertEqual(result, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60757de-f9f3-4b3a-96a4-4cf9cc165d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_list_int (__main__.TestMySum.test_list_int) ... ok\n",
      "test_tuple (__main__.TestMySum.test_tuple) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x15899a57cb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[\"\"], verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991dc50-01c5-4051-921b-65fe117217a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6da91-e21c-4390-be07-3767d724cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "When I ran my tests, the output clearly showed which cases passed and which failed—displaying “ok” for successful tests\n",
    "and “FAIL” for those that didn’t meet expectations. The passing tests confirmed that my functions worked as intended for\n",
    "the given inputs, while the failing ones revealed bugs or unhandled edge cases in my code. Each failure included an \n",
    "error message showing the expected versus actual results, along with a stack trace that helped me identify exactly where \n",
    "the issue occurred. Overall, these results not only highlight areas that need fixing but also show that my tests are doing \n",
    "their job—catching discrepancies and validating my program’s logic effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
